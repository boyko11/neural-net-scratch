{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "073d97b9",
   "metadata": {},
   "source": [
    "# Neural Net from scratch\n",
    "I watched a short clip from Lex Fridman interviewing Andrej Karpathy, so here I am building a neural network from scratch, putting in time towards my 10000 hours. By the time I reach this 10000 hours number, I may no longer be :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d6cf80",
   "metadata": {},
   "source": [
    "## Forward Prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eb3dc02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layer 1 matrix: (6, 3)\n",
      "\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "\n",
      "Hidden Layer 2 matrix: (4, 7)\n",
      "\n",
      "[[1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      "Output Layer matrix: (1, 5)\n",
      "\n",
      "[[1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_inputs = 2\n",
    "num_hidden_layers = 2\n",
    "num_nodes_hidden_layer_1 = 6\n",
    "num_nodes_hidden_layer_2 = 4\n",
    "num_outputs = 1\n",
    "\n",
    "# weight matrix between the inputs and the first hidden layer\n",
    "# If our first hidden layer has 6 units\n",
    "# we'd need to produce a calculation for each hidden unit,\n",
    "# IE the result of the dot product of the weight matrix for this hidden layer and the input vector should be\n",
    "# a 6 x 1 vector.\n",
    "# what we feed that matrix is our input layer:\n",
    "# the dimension of our input is 2 x 1, but since we also need to add a bias unit,\n",
    "# for convenience we add the bias into the input vector as a 1 - \n",
    "# [\n",
    "#    1,\n",
    "#    x1,\n",
    "#    x2\n",
    "# ]\n",
    "# Now we feed the weight matrix a (3 x 1) input vector so we get a 6 x 1 activations vector, hence\n",
    "# Our weight matrix needs to be 6 x 3 :\n",
    "# (6 x 3) dot (3 x 1) = 6 x 1\n",
    "\n",
    "# for the sake of verifying calculation, for now our weight matrices would be 1s\n",
    "# later we will randomize them\n",
    "hidden_layer_1_weight_matrix = np.ones((num_nodes_hidden_layer_1, num_inputs + 1))\n",
    "\n",
    "print(f'Hidden Layer 1 matrix: {hidden_layer_1_weight_matrix.shape}\\n')\n",
    "print(hidden_layer_1_weight_matrix)\n",
    "\n",
    "\n",
    "print()\n",
    "# Similarly, we need to feed this  6 x 1 activations vector plus a bias unit(so 7 x 1 vector)\n",
    "# into the weight matrix for the second hidden layer\n",
    "# The second hidden layer has 4 nodes, hence needs to produce a 4 x 1 activations vector, so\n",
    "# The weight matrix for the second hidden layer would be 4 x 7: (4x7) dot (7x1) = 4x1\n",
    "\n",
    "hidden_layer_2_weight_matrix = np.ones((num_nodes_hidden_layer_2, num_nodes_hidden_layer_1 + 1))\n",
    "\n",
    "print(f'Hidden Layer 2 matrix: {hidden_layer_2_weight_matrix.shape}\\n')\n",
    "print(hidden_layer_2_weight_matrix)\n",
    "\n",
    "print()\n",
    "# And again, we need to feed the 4x1 activations vector plus a bias unit(so 5x1)\n",
    "# into the weight matrix for the output layer\n",
    "# The output layer is a singled node (1, 1), so\n",
    "# The weight matrix for the output layer would be (1x5)\n",
    "# (1x5)dot(5x1) = (1x1)\n",
    "\n",
    "output_layer_weight_matrix = np.ones((num_outputs, num_nodes_hidden_layer_2 + 1))\n",
    "\n",
    "print(f'Output Layer matrix: {output_layer_weight_matrix.shape}\\n')\n",
    "print(output_layer_weight_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9af1e3e",
   "metadata": {},
   "source": [
    "### Actual Forward Prop\n",
    "##### Now that we have all the matrices, let's see if we can get a sample input vector to turn into output.\n",
    "##### We will feed a forward prop function our input vector(2x1) and we should end up with a 1x1 input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb73e94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_input shape: (2, 1)\n",
      "\n",
      "hidden_layer_1_weight_matrix shape: (6, 3)\n",
      "input_vector_plus_bias shape: (3, 1)\n",
      "layer_1_activations_vector shape: (6, 1)\n",
      "\n",
      "[[5.]\n",
      " [5.]\n",
      " [5.]\n",
      " [5.]\n",
      " [5.]\n",
      " [5.]]\n",
      "\n",
      "hidden_layer_2_weight_matrix shape: (4, 7)\n",
      "layer_1_activations_vector_plus_bias shape: (7, 1)\n",
      "layer_2_activations_vector: (4, 1)\n",
      "\n",
      "[[31.]\n",
      " [31.]\n",
      " [31.]\n",
      " [31.]]\n",
      "\n",
      "output_layer_weight_matrix shape: (1, 5)\n",
      "layer_2_activations_vector_plus_bias shape: (5, 1)\n",
      "output_vector shape: (1, 1)\n",
      "\n",
      "[[125.]]\n"
     ]
    }
   ],
   "source": [
    "def forward_prop(input_vector, hidden_layer_1_weight_matrix, hidden_layer_2_weight_matrix, \n",
    "                 output_layer_weight_matrix):\n",
    "    \n",
    "    # add the bias unit to the input\n",
    "    input_vector_plus_bias = np.vstack([np.array([1]), input_vector])\n",
    "    layer_1_activations_vector = np.dot(hidden_layer_1_weight_matrix, input_vector_plus_bias)\n",
    "    \n",
    "    print(f'\\nhidden_layer_1_weight_matrix shape: {hidden_layer_1_weight_matrix.shape}')\n",
    "    print(f'input_vector_plus_bias shape: {input_vector_plus_bias.shape}')\n",
    "    print(f'layer_1_activations_vector shape: {layer_1_activations_vector.shape}\\n')\n",
    "    print(layer_1_activations_vector)\n",
    "    \n",
    "    # add the bias unit to the layer_1_activations_vector\n",
    "    layer_1_activations_vector_plus_bias = np.vstack([np.array([1]), layer_1_activations_vector])\n",
    "    layer_2_activations_vector = np.dot(hidden_layer_2_weight_matrix, layer_1_activations_vector_plus_bias)\n",
    "    \n",
    "    print(f'\\nhidden_layer_2_weight_matrix shape: {hidden_layer_2_weight_matrix.shape}')\n",
    "    print(f'layer_1_activations_vector_plus_bias shape: {layer_1_activations_vector_plus_bias.shape}')\n",
    "    print(f'layer_2_activations_vector: {layer_2_activations_vector.shape}\\n')\n",
    "    print(layer_2_activations_vector)\n",
    "    \n",
    "    \n",
    "    # add the bias unit to the layer_2_activations_vector\n",
    "    layer_2_activations_vector_plus_bias = np.vstack([np.array([1]), layer_2_activations_vector])\n",
    "    output_vector = np.dot(output_layer_weight_matrix, layer_2_activations_vector_plus_bias)\n",
    "    \n",
    "    print(f'\\noutput_layer_weight_matrix shape: {output_layer_weight_matrix.shape}')\n",
    "    print(f'layer_2_activations_vector_plus_bias shape: {layer_2_activations_vector_plus_bias.shape}')\n",
    "    print(f'output_vector shape: {output_vector.shape}\\n')\n",
    "    \n",
    "    return output_vector\n",
    "\n",
    "sample_input = np.array([\n",
    "    [2],\n",
    "    [2]\n",
    "])\n",
    "print(f'sample_input shape: {sample_input.shape}')\n",
    "output_vector = forward_prop(sample_input, hidden_layer_1_weight_matrix, hidden_layer_2_weight_matrix, \n",
    "                 output_layer_weight_matrix)\n",
    "\n",
    "\n",
    "print(output_vector)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04479ef7",
   "metadata": {},
   "source": [
    "Seems to check out, but the activation function is the identity function => this is all very linear\n",
    "To make it non-linear, we'd have to feed the activations to non-linear functions\n",
    "##### The sigmoid is a good learning start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9dc3f6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(this_vector):\n",
    "    return 1/(1 + np.exp(-this_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d64ea3",
   "metadata": {},
   "source": [
    "The sigmoid outputs numbers in the range (0,1). For this exercise, we will attempt to create a network that can predict a number in any range, hence we will not run our output activation through the sigmoid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9763462",
   "metadata": {},
   "source": [
    "We will squish all the hidden activations through the sigmoid though. Here's the updated forward_prop function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "efc1bbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.23936876]]\n"
     ]
    }
   ],
   "source": [
    "def forward_prop(input_vector, hidden_layer_1_weight_matrix, hidden_layer_2_weight_matrix, \n",
    "                 output_layer_weight_matrix):\n",
    "    \n",
    "    input_vector_plus_bias = np.vstack([np.array([1]), input_vector])\n",
    "    layer_1_activations_vector = np.dot(hidden_layer_1_weight_matrix, input_vector_plus_bias)   \n",
    "    layer_1_activations_vector = sigmoid(layer_1_activations_vector)\n",
    "    \n",
    "    layer_1_activations_vector_plus_bias = np.vstack([np.array([1]), layer_1_activations_vector])\n",
    "    layer_2_activations_vector = np.dot(hidden_layer_2_weight_matrix, layer_1_activations_vector_plus_bias)\n",
    "    layer_2_activations_vector = sigmoid(layer_2_activations_vector)\n",
    "    \n",
    "    layer_2_activations_vector_plus_bias = np.vstack([np.array([1]), layer_2_activations_vector])\n",
    "    output_vector = np.dot(output_layer_weight_matrix, layer_2_activations_vector_plus_bias)\n",
    "    \n",
    "    return output_vector\n",
    "\n",
    "sample_input = np.array([\n",
    "    [2],\n",
    "    [2]\n",
    "])\n",
    "\n",
    "# As promised, let's make the weight matrices be not 1s\n",
    "hidden_layer_1_weight_matrix = np.random.random((num_nodes_hidden_layer_1, num_inputs + 1))\n",
    "hidden_layer_2_weight_matrix = np.random.random((num_nodes_hidden_layer_2, num_nodes_hidden_layer_1 + 1))\n",
    "output_layer_weight_matrix = np.random.random((num_outputs, num_nodes_hidden_layer_2 + 1))\n",
    "\n",
    "output_vector = forward_prop(sample_input, hidden_layer_1_weight_matrix, hidden_layer_2_weight_matrix, \n",
    "                 output_layer_weight_matrix)\n",
    "\n",
    "\n",
    "print(output_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79278e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
